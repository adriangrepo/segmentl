import torch
from torch import Tensor, einsum, nn
import torch.nn.functional as F
import numpy as np
from matplotlib import pyplot as plt
from scipy import ndimage
from scipy.ndimage import _ni_support
from scipy.ndimage.morphology import binary_erosion,\
    generate_binary_structure
from scipy.ndimage import distance_transform_edt as distance_transform
from scipy.ndimage.measurements import label, find_objects
from scipy.stats import pearsonr

from segmentl.distribution.distribution_based import CrossentropyND, TopKLoss
from segmentl.utils import get_activation_fn
from segmentl.region.region_based import SoftDiceLoss




class DistanceMapBinaryCELoss(nn.Module):
    """
    Distance map penalized CE loss
    Modified after: https://github.com/JunMa11/SegLoss/blob/master/losses_pytorch/boundary_loss.py
    Original code based on: Caliva et al. (2019) https://openreview.net/pdf?id=B1eIcvS45V

    'Error-penalizing distance maps were generated by computing the distance transform on the
    segmentation masks and then reverting them, by voxel-wise subtracting the binary segmentation
    from the mask overall max distance value.
    This  procedure  aims  to  compute  a  distance  mask  where  pixels  in proximity of the bones are weighted more,
    compared to those located far away.' (Distance Map Loss Penalty Term for Semantic Segmentation (Caliva et al. 2019))
    """

    def __init__(self, threshold=0.5, smooth=1e-5, dw_ce_wt=1.0, boundary_fill=True, debug=False):
        '''
        :param threshold:
        :param smooth:
        :param dw_dice_wt: weighting for dice loss
        :param boundary_fill: original paper used distance inside boundary, set to True to fill with mask
        :param debug: use for plotting images to files
        '''
        super(DistanceMapBinaryCELoss, self).__init__()
        self.threshold = threshold
        self.dw_ce_wt = dw_ce_wt
        self.boundary_fill=boundary_fill
        if debug:
            self.debug_name = 'DistanceMapBinaryCELoss'
        else:
            self.debug_name = ''

    def forward(self, net_output, gt):
        """
        binary net_output: (B, 1, x,y,z)
        target: ground truth, shape: (B, 1, x,y,z)
        """
        dist, y_onehot = dist_torch(net_output, gt, self.threshold, self.boundary_fill, self.debug_name)

        # combine Sigmoid+BCELoss
        loss = nn.BCEWithLogitsLoss()
        output = loss(net_output, gt)
        dw_ce = output*self.dw_ce_wt*dist

        if self.debug_name:
            i = 0
            for mi in dw_ce.detach().cpu().numpy():
                plt.imshow(mi)
                plt.savefig(f'images/{self.debug_name}_dw_ce_{i}.png')
                i += 1
        return 1 - dw_ce.mean()


class DistancePenalizedCELoss(torch.nn.Module):
    """
    after https://github.com/JunMa11/SegLoss/blob/master/losses_pytorch/ND_Crossentropy.py
    """
    def __init__(self, precision='half'):
        super(DistancePenalizedCELoss, self).__init__()
        self.precision=precision

    def forward(self, inp, target):
        # print(inp.shape, target.shape) # (batch, 2, xyz), (batch, 2, xyz)
        # compute distance map of ground truth
        with torch.no_grad():
            dist = compute_edts_forPenalizedLoss(target.cpu().numpy() > 0.5) + 1.0
            nans = np.isnan(dist)
            assert nans.sum() == 0

        dist = torch.from_numpy(dist)
        if dist.device != inp.device:
            dist = dist.to(inp.device).type(torch.float32)

        if self.precision=='half':
            dist=dist.half()

        print(f'DistancePenalizedCELoss dist: {dist.shape}')
        dist = dist.view(-1, )
        print(f'post view dist: {dist.shape}')

        target = target.long()
        num_classes = inp.size()[1]

        i0 = 1
        i1 = 2

        while i1 < len(inp.shape):  # this is ugly but torch only allows to transpose two axes at once
            inp = inp.transpose(i0, i1)
            i0 += 1
            i1 += 1

        print(f'inp transposed: {inp.shape}')
        inp = inp.contiguous()
        print(f'inp contiguous: {inp.shape}')
        if num_classes>1:
            inp = inp.view(-1, num_classes)
            log_sm = torch.nn.LogSoftmax(dim=1)
            inp_logs = log_sm(inp)
            target = target.view(-1, )
        else:
            #Binary
            activation_fn = get_activation_fn('Sigmoid')
            sig = activation_fn(inp)
            inp_logs=torch.log(sig)
            inp_logs=inp_logs.permute(0,3,1,2)

        print(f'inp_logs: {inp_logs.shape}, target: {target.shape}')
        # loss = nll_loss(inp_logs, target)
        loss = -inp_logs[range(target.shape[0]), target]
        # TODO fix IndexError: shape mismatch: indexing tensors could not be broadcast together with shapes [32], [32, 1, 256, 256]
        print(f'loss.type(): {type(loss)}, dist.type(): {type(dist)}, loss: {loss.shape}, dist: {dist.shape}')
        weighted_loss = loss * dist

        return weighted_loss.mean()





def generate_boundary(net_output, target, debug):
    # This needs work  - use skimage boundary function instead
    np_tgt = target.detach().cpu().numpy()
    distance = distance_transform(np_tgt)
    distance[distance != 1] = 0
    b = np.where(distance == 1)
    print(f'b: {b}, type: {type(b)},  distance: {type(distance)}')

    if debug:
        for i, j in enumerate(distance):
            print(j.shape)
            plt.imshow(j.view(256, -1))
            plt.savefig(f'images/generate_boundary_{i}.png')
    bound = torch.from_numpy(b).to(net_output.device)
    return bound

def one_hot2dist_binary(seg: np.ndarray) -> np.ndarray:
    # modified after https://github.com/LIVIAETS/surface-loss/blob/master/utils.py

    res = np.zeros_like(seg)
    posmask = seg.astype(np.bool)

    if posmask.any():
        negmask = ~posmask
        res = distance_transform(negmask) * negmask - (distance_transform(posmask) - 1) * posmask
    return res

def calc_dist_maps(gt, n_class=1):
    '''
        lambda img: np.array(img)[np.newaxis, ...],
        lambda nd: torch.tensor(nd, dtype=torch.int64),
        partial(class2one_hot, C=n_class),
        itemgetter(0),
        lambda t: t.cpu().numpy(),
        one_hot2dist,
        lambda nd: torch.tensor(nd, dtype=torch.float32)
    '''
    t=gt.cpu().numpy()
    #gt = gt.type(torch.int32)
    #to=class2one_hot(gt, n_class)
    td=one_hot2dist_binary(t)
    nd=torch.tensor(td, dtype=torch.float32).to(gt.device)
    return nd

class SurfaceLoss(nn.Module):
    '''
    after https://github.com/LIVIAETS/surface-loss/blob/master/losses.py
    '''
    def __init__(self, apply_nonlin='Sigmoid'):
        super(SurfaceLoss, self).__init__()
        self.apply_nonlin=apply_nonlin

    def forward(self, pred, gt):
        activation_fn = get_activation_fn(self.apply_nonlin)
        pred = activation_fn(pred)

        dist_maps=calc_dist_maps(gt)

        pc = pred[:, ...].type(torch.float32)
        dc = dist_maps[:, ...].type(torch.float32)

        multipled = einsum("bcwh,bcwh->bcwh", pc, dc)

        loss = multipled.mean()
        print(f'SurfaceLoss: {loss}')
        return loss

#testing code
def gen_sample():
    logits = torch.randn(4, 1, 768, 768).cuda()
    logits = torch.tensor(logits, requires_grad=True)
    scores = torch.softmax(logits, dim=1)
    # print(scores)
    labels = torch.randint(0, 1, (4, 1, 768, 768)).cuda()
    print(f'scores: {scores.shape}, labels: {labels.shape}')
    labels[0, 30:35, 40:45] = 1
    labels[1, 0:5, 40:45] = 1
    # print(labels)
    return scores, labels

def run_dist():
    criteria = DistanceMapDiceBinaryLoss()
    criteria.cuda()
    logits = torch.randn(4, 1, 768, 768).cuda()
    logits = torch.tensor(logits, requires_grad=True)
    scores = torch.softmax(logits, dim=1)
    # print(scores)
    labels = torch.randint(0, 1, (4, 1, 768, 768)).cuda()
    print(f'scores: {scores.shape}, labels: {labels.shape}')
    labels[0, 30:35, 40:45] = 1
    labels[1, 0:5, 40:45] = 1
    # print(labels)

    loss = criteria(scores, labels)
    print(f'loss: {loss}')
    loss.backward()

if __name__ == '__main__':
    scores, labels = gen_sample()
    run_dist(scores, labels)